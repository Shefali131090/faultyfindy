{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXf_l66SWYsu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "import zipfile\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWtmZ89LdiYT",
        "outputId": "f481e15d-306a-4fc2-b19c-485e1202c267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Extract the uploaded zip file\n",
        "for fn in uploaded.keys():\n",
        "  if fn.endswith('.zip'):\n",
        "    with zipfile.ZipFile(fn, 'r') as zip_ref:\n",
        "      zip_ref.extractall('.')\n",
        "\n",
        "# Update dataset_path to reflect the uploaded files' location\n",
        "dataset_path = '.'\n",
        "print(\"Digital images of defective and good condition tyres:\")\n",
        "print(os.listdir(dataset_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "XF1ySPc-WboW",
        "outputId": "d9fb8057-3518-48bd-ec55-5463c5edfded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a9862692-893b-4907-919a-c9380443b210\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a9862692-893b-4907-919a-c9380443b210\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digital images of defective and good condition tyres:\n",
            "['.config', 'drive', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the path to dataset folder\n",
        "dataset_path = '/content/drive/My Drive/Digital images of defective and good condition tyres'\n",
        "\n",
        "# Verify the folder\n",
        "import os\n",
        "print(\"Dataset directory found at:\", dataset_path)\n",
        "print(\"Contents of the dataset folder:\", os.listdir(dataset_path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWoe8mq6Y1jZ",
        "outputId": "3449bbbd-cbd7-4af3-c39c-556ac9e72953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dataset directory found at: /content/drive/My Drive/Digital images of defective and good condition tyres\n",
            "Contents of the dataset folder: ['desktop.ini', 'good', 'defective']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Paths to the subdirectories for good and defective tyres\n",
        "good_tyres_path = '/content/drive/My Drive/Digital images of defective and good condition tyres/good'\n",
        "defective_tyres_path = '/content/drive/My Drive/Digital images of defective and good condition tyres/defective'\n",
        "\n",
        "def verify_images(folder_path):\n",
        "    valid_images = 0\n",
        "    invalid_images = 0\n",
        "    invalid_files = []\n",
        "\n",
        "    # Check each image file in the folder\n",
        "    for image_name in os.listdir(folder_path):\n",
        "        image_path = os.path.join(folder_path, image_name)\n",
        "        try:\n",
        "            # Try to open the image to check if it's valid\n",
        "            with Image.open(image_path) as img:\n",
        "                img.verify()  # Verify if the image is valid\n",
        "            valid_images += 1\n",
        "        except (IOError, SyntaxError):\n",
        "            # If there is an error, it's an invalid image\n",
        "            invalid_images += 1\n",
        "            invalid_files.append(image_name)\n",
        "\n",
        "    return valid_images, invalid_images, invalid_files\n",
        "\n",
        "# Verify images in both folders\n",
        "valid_good, invalid_good, invalid_good_files = verify_images(good_tyres_path)\n",
        "valid_defective, invalid_defective, invalid_defective_files = verify_images(defective_tyres_path)\n",
        "\n",
        "print(f\"Good Tyres - Valid: {valid_good}, Invalid: {invalid_good}\")\n",
        "print(f\"Defective Tyres - Valid: {valid_defective}, Invalid: {invalid_defective}\")\n",
        "\n",
        "if invalid_good > 0:\n",
        "    print(f\"Invalid good tyre images: {invalid_good_files}\")\n",
        "\n",
        "if invalid_defective > 0:\n",
        "    print(f\"Invalid defective tyre images: {invalid_defective_files}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot1HThu-e8bY",
        "outputId": "d1f7c8c4-772b-4b0c-b45a-ee6808e5b2af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good Tyres - Valid: 91, Invalid: 0\n",
            "Defective Tyres - Valid: 0, Invalid: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "def verify_images_opencv(folder_path):\n",
        "    valid_images = 0\n",
        "    invalid_images = 0\n",
        "    invalid_files = []\n",
        "\n",
        "    for image_name in os.listdir(folder_path):\n",
        "        image_path = os.path.join(folder_path, image_name)\n",
        "\n",
        "        try:\n",
        "            # Try reading the image using OpenCV\n",
        "            img = cv2.imread(image_path)\n",
        "\n",
        "            # Check if the image was read properly\n",
        "            if img is not None:\n",
        "                valid_images += 1\n",
        "            else:\n",
        "                invalid_images += 1\n",
        "                invalid_files.append(image_name)\n",
        "\n",
        "        except Exception as e:\n",
        "            # Handle any exceptions (corrupt files, etc.)\n",
        "            invalid_images += 1\n",
        "            invalid_files.append(image_name)\n",
        "\n",
        "    return valid_images, invalid_images, invalid_files\n",
        "\n",
        "# Verify the images in the 'defective' folder again\n",
        "defective_tyres_path = '/content/drive/My Drive/Digital images of defective and good condition tyres/defective'\n",
        "valid_defective, invalid_defective, invalid_defective_files = verify_images_opencv(defective_tyres_path)\n",
        "\n",
        "print(f\"Defective Tyres - Valid: {valid_defective}, Invalid: {invalid_defective}\")\n",
        "if invalid_defective > 0:\n",
        "    print(f\"Invalid defective tyre images: {invalid_defective_files}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ji7jK1Azf3FN",
        "outputId": "3db5caa7-c36b-435c-b5a2-c3258ea9420e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defective Tyres - Valid: 272, Invalid: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Define the image size\n",
        "IMG_SIZE = (128, 128)  # Resize images to 128x128 pixels\n",
        "\n",
        "# Set up ImageDataGenerator for augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,               # Normalize pixel values to range [0, 1]\n",
        "    rotation_range=40,            # Random rotations\n",
        "    width_shift_range=0.2,        # Random horizontal shifts\n",
        "    height_shift_range=0.2,       # Random vertical shifts\n",
        "    shear_range=0.2,              # Random shearing\n",
        "    zoom_range=0.2,               # Random zoom\n",
        "    horizontal_flip=True,         # Random horizontal flip\n",
        "    fill_mode='nearest'           # Fill missing pixels after transformation\n",
        ")\n",
        "\n",
        "# Create a function to load and preprocess the images\n",
        "def load_and_preprocess_images(folder_path, image_size=IMG_SIZE):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for label, subfolder in enumerate(['good', 'defective']):\n",
        "        subfolder_path = os.path.join(folder_path, subfolder)\n",
        "\n",
        "        for img_name in os.listdir(subfolder_path):\n",
        "            img_path = os.path.join(subfolder_path, img_name)\n",
        "\n",
        "            try:\n",
        "                # Load and resize the image\n",
        "                img = image.load_img(img_path, target_size=image_size)\n",
        "                img_array = image.img_to_array(img)\n",
        "                images.append(img_array)\n",
        "                labels.append(label)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image {img_name}: {e}\")\n",
        "\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "# Load and preprocess the images\n",
        "dataset_path = '/content/drive/My Drive/Digital images of defective and good condition tyres'\n",
        "X, y = load_and_preprocess_images(dataset_path)\n",
        "\n",
        "# Apply data augmentation on the images\n",
        "augmented_images = datagen.flow(X, y, batch_size=32)\n",
        "\n",
        "print(\"Preprocessing completed! Ready for training.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww9nXfxHgRqM",
        "outputId": "1352f548-5656-4287-96fc-3c46e79bb879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing completed! Ready for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential()\n",
        "\n",
        "# First convolutional layer\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))  # 128x128 images with 3 channels (RGB)\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Second convolutional layer\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Third convolutional layer\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flatten the output from the convolutional layers\n",
        "model.add(Flatten())\n",
        "\n",
        "# Fully connected layer\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n",
        "\n",
        "# Output layer (2 classes: good or defective)\n",
        "model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(augmented_images, steps_per_epoch=len(X) // 32, epochs=10, verbose=1)\n",
        "\n",
        "# Save the model after training\n",
        "model.save('tyre_fault_model.h5')\n",
        "\n",
        "print(\"Model training complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YIPm4jOhWLF",
        "outputId": "3ef7640a-22de-4ce8-ebc4-230a70f72e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 1s/step - accuracy: 0.5146 - loss: 0.7575\n",
            "Epoch 2/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.5938 - loss: 0.6779\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.5469 - loss: 0.6781\n",
            "Epoch 4/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 423us/step - accuracy: 0.6250 - loss: 0.6100 \n",
            "Epoch 5/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.6385 - loss: 0.6498\n",
            "Epoch 6/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 451us/step - accuracy: 0.6562 - loss: 0.6549 \n",
            "Epoch 7/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.6350 - loss: 0.6391\n",
            "Epoch 8/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 455us/step - accuracy: 0.7188 - loss: 0.5874 \n",
            "Epoch 9/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - accuracy: 0.6629 - loss: 0.6123\n",
            "Epoch 10/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 403us/step - accuracy: 0.6562 - loss: 0.6156 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('tyre_fault_model.keras')\n"
      ],
      "metadata": {
        "id": "4uCh_ivjhygs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "# Path to the dataset folder\n",
        "dataset_path = '/content/drive/My Drive/Digital images of defective and good condition tyres'\n",
        "\n",
        "# Load images and their corresponding labels\n",
        "def load_images_from_folder(folder, label):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(folder):\n",
        "        img = load_img(os.path.join(folder, filename), target_size=(128, 128))\n",
        "        if img is not None:\n",
        "            img_array = img_to_array(img) / 255.0  # Normalize the images\n",
        "            images.append(img_array)\n",
        "            labels.append(label)\n",
        "    return images, labels\n",
        "\n",
        "# Load images from both \"good\" and \"defective\" folders\n",
        "good_images, good_labels = load_images_from_folder(os.path.join(dataset_path, 'good'), 0)  # 0 for good\n",
        "defective_images, defective_labels = load_images_from_folder(os.path.join(dataset_path, 'defective'), 1)  # 1 for defective\n",
        "\n",
        "# Combine the images and labels\n",
        "all_images = np.array(good_images + defective_images)\n",
        "all_labels = np.array(good_labels + defective_labels)\n",
        "\n",
        "# Split the data into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(all_images, all_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert into tensorflow datasets if needed, or use as input for the model\n",
        "train_data = (X_train, y_train)\n",
        "test_data = (X_test, y_test)\n"
      ],
      "metadata": {
        "id": "oS2X92_vi3E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Initialize the ImageDataGenerator for normalization and augmentation\n",
        "datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "\n",
        "# Generate data batches for training and testing\n",
        "train_data = datagen.flow(X_train, y_train, batch_size=32)\n",
        "test_data = datagen.flow(X_test, y_test, batch_size=32)\n"
      ],
      "metadata": {
        "id": "-i5M1kDci6jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_data, epochs=10, validation_data=test_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK-47SzOj8kV",
        "outputId": "f2aec29a-5040-4a07-e1b9-7921bf48ee2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.5532 - loss: 2.1622 - val_accuracy: 0.4439 - val_loss: 0.6946\n",
            "Epoch 2/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 1s/step - accuracy: 0.4521 - loss: 0.6942 - val_accuracy: 0.5561 - val_loss: 0.6927\n",
            "Epoch 3/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.4959 - loss: 0.6925 - val_accuracy: 0.5561 - val_loss: 0.6908\n",
            "Epoch 4/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.5218 - loss: 0.6899 - val_accuracy: 0.5561 - val_loss: 0.6879\n",
            "Epoch 5/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - accuracy: 0.5604 - loss: 0.6893 - val_accuracy: 0.5561 - val_loss: 0.6875\n",
            "Epoch 6/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.5589 - loss: 0.6917 - val_accuracy: 0.5561 - val_loss: 0.6879\n",
            "Epoch 7/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1s/step - accuracy: 0.5854 - loss: 0.6851 - val_accuracy: 0.5561 - val_loss: 0.6870\n",
            "Epoch 8/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.5764 - loss: 0.6851 - val_accuracy: 0.5561 - val_loss: 0.6869\n",
            "Epoch 9/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1s/step - accuracy: 0.5870 - loss: 0.6846 - val_accuracy: 0.5561 - val_loss: 0.6868\n",
            "Epoch 10/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.6002 - loss: 0.6759 - val_accuracy: 0.5561 - val_loss: 0.6868\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7bd40aaf5780>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_data, verbose=1)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "print(f\"Test Loss: {test_loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q_QimpIkBqR",
        "outputId": "216e7ce8-6f7f-461f-af23-e3f4fc698766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 401ms/step - accuracy: 0.5389 - loss: 0.6904\n",
            "Test Accuracy: 0.5561224222183228\n",
            "Test Loss: 0.6868442893028259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255.0,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n"
      ],
      "metadata": {
        "id": "90E3nB_UnASz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CC2j1iXznOjp",
        "outputId": "94e5286e-b51c-4f7e-b161-8fa812a7f7dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get predictions from the model\n",
        "y_pred = model.predict(test_data)\n",
        "y_pred = (y_pred > 0.5).astype(\"int32\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Good', 'Defective'], yticklabels=['Good', 'Defective'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        },
        "id": "tkPIDaFZnSUf",
        "outputId": "40aaf36d-0ae4-42f5-f666-e2ea05e57eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 6s/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      1.00      0.71       109\n",
            "           1       0.00      0.00      0.00        87\n",
            "\n",
            "    accuracy                           0.56       196\n",
            "   macro avg       0.28      0.50      0.36       196\n",
            "weighted avg       0.31      0.56      0.40       196\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = {0: 1, 1: 5}  # Assign a higher weight to the minority class (class 1)\n",
        "model.fit(train_data, epochs=10, class_weight=class_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm9Ie6mUnYhH",
        "outputId": "b69633b8-8156-4a97-b8a8-c70ed63242c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 7s/step - accuracy: 0.4492 - loss: 3.6801\n",
            "Epoch 2/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 6s/step - accuracy: 0.4323 - loss: 1.4633\n",
            "Epoch 3/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 7s/step - accuracy: 0.4110 - loss: 1.4142\n",
            "Epoch 4/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 7s/step - accuracy: 0.4669 - loss: 1.3971\n",
            "Epoch 5/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 7s/step - accuracy: 0.4159 - loss: 1.4079\n",
            "Epoch 6/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 7s/step - accuracy: 0.4294 - loss: 1.4340\n",
            "Epoch 7/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 7s/step - accuracy: 0.4587 - loss: 1.4061\n",
            "Epoch 8/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 6s/step - accuracy: 0.4193 - loss: 1.5144\n",
            "Epoch 9/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 6s/step - accuracy: 0.4160 - loss: 1.4206\n",
            "Epoch 10/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 6s/step - accuracy: 0.4357 - loss: 1.4198\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7bd40aaf44f0>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = (model.predict(test_data) > 0.4).astype(\"int32\")  # Lowering the threshold to 0.4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Iuu29uToFxu",
        "outputId": "3b2a2954-3a25-42e1-aca1-a9341b960feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tYZu1ftVvTCm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}